# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gUtL3-z4uKhwMlaBHsFOq8Wb3kIKWm_5
"""

from __future__ import print_function
from cmath import nan
from os import truncate

from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.types import IntegerType,StringType
from pyspark.sql.functions import col,isnan,when,count,lower,regexp_replace,udf
from pyspark.ml.feature import Imputer,Tokenizer,StopWordsRemover,HashingTF, IDF, CountVectorizer, VectorAssembler, RegexTokenizer
from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,MultilayerPerceptronClassifier,LinearSVC
from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
#from langdetect import detect
from textblob import TextBlob



# import nltk
# from nltk.corpus import stopwords
import re

# pip install pyspark

spark = SparkSession.builder.appName('hw3').getOrCreate()
data = spark.read.csv("./english.csv", sep=',', multiLine=True,header = True)

data = data.filter((data.lang == 'en'))

data.groupBy('lang').count().show()

def sentiment_analysis(tweet):
    # Determine polarity
        polarity = TextBlob(" ".join(tweet)).sentiment.polarity
    # Classify overall sentiment
        if polarity > 0:
        # positive
            sentiment = 1
        elif polarity == 0:
        # neutral
            sentiment = 0
        else:
        # negative
             sentiment = 0
        return sentiment

polarityUDF= udf(lambda x: sentiment_analysis(x),StringType())
    #tweet=tweet.select(polarityUDF(col("tweet")).alias("tweet") )
tweet=data.withColumn("sentiment", polarityUDF("tweet"))

# tweet = tweet.sampleBy("sentiment", fractions={"1": 1, "2": 0.01}, seed=0)

tweet.groupBy('sentiment').count().show()

## preprocessing on the data and removing extra columns

tweet = tweet.select((lower(regexp_replace('tweet', "[^a-zA-Zd+ ]", "")).alias('tweet')),'sentiment')

tokenizer = Tokenizer(inputCol="tweet", outputCol="tweet1")
data1 = tokenizer.transform(tweet)
remover = StopWordsRemover(inputCol="tweet1", outputCol="tweet2")
data1=remover.transform(data1)

data1.show()

cols_redundant = ["tweet","tweet1"]
sampled_data = data1.drop(*cols_redundant)

# from google.colab import drive
# drive.mount('/drive')

# !cp -r LDAModel/ "/content/drive/My Drive/"

sampled_data.show()

from pyspark.ml.feature import Imputer,Tokenizer,StopWordsRemover,HashingTF, IDF, CountVectorizer, VectorAssembler, RegexTokenizer
# cv = CountVectorizer()
# cv.setVocabSize(100000)
# cv.setInputCol("tweet2")
# cv.setOutputCol("features")
# model = cv.fit(sampled_data)
# tweet_vocab_length = len(model.vocabulary)

# tweet_vocab_length

# sampled_data.show()

# cvdata=model.transform(sampled_data)


## converting data to tf idf vectors

hashingTF =HashingTF(inputCol="tweet2",outputCol="features1",numFeatures=25000)
featurizedData = hashingTF.transform(sampled_data)
idf=IDF(inputCol="features1",outputCol="features")
idfModel=idf.fit(featurizedData)
final_data=idfModel.transform(featurizedData)

final_data.show()





## Logistic regression using tf idf data without LDA
print("!"*20,"fitting lr model on tf idf data","!"*20)
data_temp = final_data.sampleBy(col("sentiment"), fractions={0: 1, 1: 0.01}, seed=0)
(trainingData, testData) = data_temp.randomSplit([0.7,0.3],seed = 1)
trainingData.show()
print("------------------test data---------------")
testData.show()
trainingData = trainingData.withColumnRenamed("sentiment","label")
cols = ("sentiment","tweet1","tweet2","features1")
trainingData = trainingData.drop(*cols)
trainingData = trainingData.withColumn("label",col("label").cast(IntegerType()))

lr = LogisticRegression()
lr.setFeaturesCol('features')
lr.setLabelCol('label')

lr_paramGrid = ParamGridBuilder() \
            .addGrid(lr.maxIter,[10])\
            .addGrid(lr.regParam, [0.2]) \
            .build()

crossval = CrossValidator(estimator=lr,
                           estimatorParamMaps=lr_paramGrid,
                           evaluator=BinaryClassificationEvaluator(),
                           numFolds=2)
# mdoel_lr = lr.fit(trainingData)
# predictions = mdoel_lr.transform(testData)

best_model = crossval.fit(trainingData)
predictions = best_model.transform(testData)
predictions.show()
evaluator = BinaryClassificationEvaluator(labelCol="label")
evaluator.setRawPredictionCol("prediction")

print(evaluator.evaluate(predictions))





#cvdata.show()

# ldaData=featurizedData.select(col("tweet"))

from pyspark.ml.linalg import Vectors, SparseVector
from pyspark.ml.clustering import LDA
lda = LDA(k=10, seed=1, optimizer="em")
lda.setMaxIter(10)
ldamodel = lda.fit(final_data)

# lda_path = "./content/lda150000/metadata/part-00000"
#lda.save(lda_path)

# ldamodel= LDA.load(lda_path)

ldamodel.describeTopics().show(truncate=False)

ldamodel.topicsMatrix()

ldamodel.explainParams()

# ldamodel.logPerplexity(cvdata)

from pyspark.sql import functions as F
vocab = ldamodel.vocabulary
def get_words(token_list):
    return [vocab[token_id] for token_id in token_list]
udf_to_words = F.udf(get_words, StringType())

num_top_words = 50
topics = ldamodel.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))
topics.select('topic', 'topicWords').show(truncate=False)



transformed = ldamodel.transform(final_data)

transformed.show()

# ldamodel.save('./LDAModel')

transformed = transformed.withColumn("sentiment",col("sentiment").cast(IntegerType()))

# transformed

transformed = transformed.sampleBy("sentiment", fractions={0: 0.01, 1: 1}, seed=0)

transformed.groupBy('sentiment').count().show()

transformed_columns = transformed.columns
transformed_columns.remove("sentiment")
transformed_columns.remove("tweet2")
assembler = VectorAssembler(inputCols = transformed_columns,outputCol = "features1")
data_final = assembler.transform(transformed)

data_final

(trainingData, testData) = data_final.randomSplit([0.7,0.3],seed = 0)
print("!"*20,"fitting lr model on tf idf data with LDA","!"*20)
lr = LogisticRegression()
lr.setLabelCol("sentiment")
lr.setFeaturesCol("features1")

paramGrid = ParamGridBuilder() \
    .addGrid(lr.maxIter,[20])\
    .addGrid(lr.regParam, [0.2]) \
    .build()

crossval = CrossValidator(estimator=lr,
                           estimatorParamMaps=paramGrid,
                           evaluator=BinaryClassificationEvaluator(labelCol = "sentiment"),
                           numFolds=2)

cvModel = crossval.fit(trainingData)
prediction = cvModel.transform(testData)

evaluator = BinaryClassificationEvaluator(labelCol = "sentiment")
print(evaluator.evaluate(prediction))